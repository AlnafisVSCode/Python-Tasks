{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECS7020P_Lab03.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sHQZ2xEiaxmH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHQZ2xEiaxmH"
      },
      "source": [
        "# Lab 3: Operations on NumPy arrays\n",
        "\n",
        "Many machine learning algorithms need data to be represented as **arrays**. Once our data are represented as arrays, they are **operated** on to either build solutions during the learning stage or to make predictions during the deployment stage.\n",
        "\n",
        "In Lab 2 we introduced **NumPy arrays**. NumPY arrays are a convenient way to represent homogeneous datasets, i.e. datasets where all the attributes are of the same type, for instance `float` or `int`. In this lab, we will explore some of the most important **mathematical operations** on NumPy arrays. These operations will allow us to implement solutions based on **matrix algebra** and are known as **vectorised operations**, as they operate on a whole array (*vectors*) rather than on individual values at a time.\n",
        "\n",
        "The main operations that we will cover include:\n",
        "- Arithmetic operations (addition, subtraction, etc)\n",
        "- Matrix transposition\n",
        "- Matrix multiplication\n",
        "- Matrix inversion\n",
        "\n",
        "We will use these operations to obtain the least squares solution of several linear and polynomial models. **Before attempting the lab**, make sure that you have read and understood the notes on mathematical notation and basic maths and have reviewed the lecture notes on regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzK5lzJ1bKAb"
      },
      "source": [
        "# Numerical predictions with a polynomial model: A simple example\n",
        "\n",
        "Consider the following cubic model:\n",
        "\n",
        "$\\begin{equation}\n",
        "f(x) = -2-\\frac{3}{2}x+\\frac{3}{4}x^2+\\frac{1}{4}x^3\n",
        "\\end{equation}$\n",
        "\n",
        "Assume that this model is used to predict the value of a label $y$ given a predictor $x$, i.e. assume $\\hat{y} = f(x)$. Let's use Python to calculate our model's prediction for three values of $x$, say $x=0, 1, 2$. We will start by implementing a **non-vectorised** approach:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXtQtv7sdkle"
      },
      "source": [
        "y_0 = -2 - 0*3/2 + 0**2*3/4 + 0**3*1/4\n",
        "print(\"The prediction for x=0 is:\", y_0)\n",
        "y_1 = -2 - 1*3/2 + 1**2*3/4 + 1**3*1/4\n",
        "print(\"The prediction for x=1 is:\", y_1)\n",
        "y_2 = -2 - 2*3/2 + 2**2*3/4 + 2**3*1/4\n",
        "print(\"The prediction for x=2 is:\", y_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vxfGbzYe9kZ"
      },
      "source": [
        "Do you understand how we have calculated the predictions $f(0)$, $f(1)$ and $f(2)$? Note that in Python, `*` denotes multiplication and `**` is used to calculate the power of a number, for instance `2**3` calculates $2^3$.\n",
        "\n",
        "We have written down one line of code per prediction. When we have many predictions to make, writing a line of code per prediction is less than ideal. Using loops might seem like a good option that avoids writing many repetitive lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HURoUS8Pnto"
      },
      "source": [
        "for x in [0, 1, 2]:\n",
        "  y = -2 - x*3/2 + x**2*3/4 + x**3*1/4\n",
        "  print(\"The prediction for x=\", x ,\"is:\", y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDsF0U_APoHe"
      },
      "source": [
        "However this approach is still **computationally inefficient**, as it proceeds by computing one prediction at a time. The most efficient approach is to take advantage of **vectorised operations**, where collections of values are operated on at the same time. Let's see how this would work for our simple prediction example.\n",
        "\n",
        "The first step is to import NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LM7kjMcgM6E"
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=3) # We do this to print up to 3 decimal places "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLC1n1m3haDD"
      },
      "source": [
        "We will use **vectorised arithmetic operations**. These operations are represented in the same way as the corresponding arithmetic operations for scalar numbers (`+`, `-`, `*`, `/`, `**`), but operate on NumPy arrays rather than individual values. Arithmetic operations act **elementwise**, i.e. the value of the $n$-th entry of the returned array depends on the values of the $n$-th entries of the input arrays only. The following cell illustrates how elementwise arithmetic operations work: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRX6561nhqPs"
      },
      "source": [
        "a = np.array([1, 3])\n",
        "b = np.array([2,5])\n",
        "print(\"The sum a+b is equal to\", a+b)\n",
        "print(\"The square of b is\", b**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES25sUdViXYR"
      },
      "source": [
        "We can now apply the model $f(x)$ to a NumPy array consisting of the predictor values $x=0, 1, 2$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aBdt-QDhLI1"
      },
      "source": [
        "x = np.array([[0],[1],[2]]) # x is a 1x3 column vector: 1 column and 3 rows\n",
        "print(\"The predictors x are\\n\", x)\n",
        "print(\"The square of x is\\n\", x**2)\n",
        "print(\"The cube of x is\\n\", x**3)\n",
        "y = -2 - x*3/2 + x**2*3/4 + x**3*1/4\n",
        "print(\"The predicted labels are:\\n\", y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw2bNcuwjEYd"
      },
      "source": [
        "We have not just reduced the number of lines of code (which is great, by the way). The line of code `y = -2 - x*3/2 + x**2*3/4 + x**3*1/4` operates on a NumPy array of dimensions 1x3, rather than operating 3 times on different values. Importantly, the NumPy array `x` can be of any size. In the following cell we create a vector `x` consisting of 100 values between -4 and 4 and use exactly the same line of code as above to obtain the corresponding predictions: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2eeDC3Bi3Kk"
      },
      "source": [
        "x = np.linspace(-4,4, 100)  \n",
        "print(\"The predictors x are:\", x)\n",
        "y = -2-x*3/2+x**2*3/4+x**3*1/4\n",
        "print(\"The predicted labels are:\", y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Cqy2XaUj7Np"
      },
      "source": [
        "No matter what the size of `x` is, the code that we need to write is exactly the same. Printing all the values is **not a good idea**, though (after all, **who's going to read them**?). It is much more informative to plot the label `y` against the predictor `x`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHe5W9Z4j-rd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x, y, 'r.')\n",
        "plt.xlabel(\"x (predictor)\", fontsize=18)\n",
        "plt.ylabel(\"y (label)\", fontsize=18)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l22N0W8lbsLw"
      },
      "source": [
        "Here is something **worth remembering**. In machine learning we **represent our dataset using arrays** and our algorithms are implemented as **operations on arrays**. In the next sections we will explore an even better way to express calculations such as our cubic model $f(x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W5gWMELl2gu"
      },
      "source": [
        "# Basic operations on arrays\n",
        "\n",
        "The previous example should have persuaded you that working with arrays of values is better than working with each value separately. In this section we will review some of the most important operations on arrays.\n",
        "\n",
        "First, we will use `np.array` to create a **row vector** `x_r`, specifically a vector consisting of 1 row and 3 columns, and then we will use `np.transpose` to **transpose** it into a **column vector** `x_c` consisting of 3 rows and 1 column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4-UHgXUtOpP"
      },
      "source": [
        "x_r = np.array([0, 1, 2], ndmin=2)\n",
        "print(\"Vector x_r is:\\n\", x_r)\n",
        "print(\"The shape of x_r is :\", x_r.shape)\n",
        "x_c = np.transpose(x_r)\n",
        "print(\"\\nVector x_c is:\\n\", x_c)\n",
        "print(\"The shape of x_c is:\", x_c.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UncbEjhKS8C_"
      },
      "source": [
        "Note that we could have used `x_c = np.array([[0], [1], [2]])` to produce a column vector directly, instead of producing a row vector and then using transposition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq7rgwQLvYh9"
      },
      "source": [
        "Let's now use **column stacking** to create a **matrix** X consisting of 4 columns, specifically a column of ones and 3 columns storing the powers of `x_c` up to degree 3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo4BASi3vtcq"
      },
      "source": [
        "X = np.column_stack([np.ones(x_c.shape), x_c, x_c**2, x_c**3])\n",
        "print(\"The design matrix X is:\\n\", X)\n",
        "print(\"\\nThe shape of the matrix X is\", X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUp8MImWv9OA"
      },
      "source": [
        "And finally, some magic. In the following cell, we create a **column vector** `w_c` containing the coefficients of our cubic model $f(x)$ and obtain the predictions for `x_c` by calculating the matrix multiplication between `X` and `w_c` using `np.dot`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU4NF3XowUtw"
      },
      "source": [
        "w_c = np.array([[-2], [-3/2], [3/4], [1/4]])\n",
        "print(\"The coefficients of the polynomial model are:\\n\", w_c)\n",
        "y = np.dot(X,w_c)\n",
        "print(\"The predicted labels are:\\n\", y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CggDtMPkfHc9"
      },
      "source": [
        "Note that `X` represents a 3x4 matrix, `w_c` a 4x1 vector, and the output `y` is a 3x1 vector. What if we have 100 predictors? Same code, we first create a 100x4 NumPy matrix containing the powers of all the predictors and multiply it by the 4x1 coefficients vector, which returns a 100x1 prediction vector. This time, we won't print out all the predictions, as we have learnt it's not very useful:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2bEd20gJL6"
      },
      "source": [
        "x_r = np.linspace(-4,4,100)\n",
        "x_c = np.transpose(x_r)\n",
        "X = np.column_stack([np.ones(x_c.shape), x_c, x_c**2, x_c**3])\n",
        "y = np.dot(X,w_c)\n",
        "plt.plot(x_c, y, 'r.')\n",
        "plt.xlabel(\"x (predictor)\", fontsize=18)\n",
        "plt.ylabel(\"y (label)\", fontsize=18)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv7oFht_hTft"
      },
      "source": [
        "In summary:\n",
        "\n",
        "- Our predictors have been represented as a **vector**.\n",
        "- We have created a **matrix** containing as columns an all-ones column and the powers of the predictor vector up to degree 3. Note that this matrix looks like the least squares design matrix! They are indeed identical.\n",
        "- We have represented the coefficients of $f(x)$ as a **vector**.\n",
        "- We have obtained the predicted labels using **matrix multiplication**.\n",
        "\n",
        "This is the **pro** way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvmupcJFiXSH"
      },
      "source": [
        "# Least squares\n",
        "\n",
        "Given a family of regression models, the least squares solution is the model that minimises the mean squared error on our training dataset. Consider the folowing **multiple linear model**:\n",
        "\n",
        "$\\begin{equation}\n",
        "f(x) = w_0 + w_1 x_1 + ... + w_K x_K\n",
        "\\end{equation}$\n",
        "\n",
        "where $x_1, ..., x_K$ are the predictors and $w_0, ..., w_K$ are the model's parameters. If we have a dataset consisting of $N$ samples, we can obtain the parameters of the least squares solution using the **normal equations** \n",
        "\n",
        "$\\begin{equation}\n",
        "\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
        "\\end{equation}$\n",
        "\n",
        "where $\\mathbf{w}=[w_1, ... , w_K]^T$ are the parameters of the model, $\\mathbf{y}=[y_0, ..., y_N]^T$ are the true labels in the dataset and $\\mathbf{X}$ is the design matrix. The least squares solution for **simple polynomial regression** can be obtained following an identical approach. Given a polynomial model\n",
        "\n",
        "$\\begin{equation}\n",
        "f(x) = w_0 + w_1 x + ... + w_K x^K\n",
        "\\end{equation}$\n",
        "\n",
        "we can treat $x, x^2, ..., x^K$ as separate predictors, build the corresponding design matrix $\\mathbf{X}$ and use the normal equation.\n",
        "\n",
        "As you can see from the normal equations, obtaining the least square solution involves: \n",
        "\n",
        "- **Arrays** ($\\mathbf{w}$, $\\mathbf{y}$ and $\\mathbf{X}$).\n",
        "- **Transposition**.\n",
        "- **Matrix inversion**.\n",
        "- **Matrix multiplication**. \n",
        "\n",
        "Let's explore a simple regression problem. First, we will define and plot our training dataset, which consists 10 samples described by one predictor `x` and one label `y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45EJiGfDiWRV"
      },
      "source": [
        "x = np.array([0.3000, -0.7700, 0.9000, -0.0400, 0.7400, -0.5800, -0.9200, -0.2100, -0.5400, 0.6800], ndmin=2).T\n",
        "y = np.array([1.1492,  0.3582, 1.9013,  0.9487, 1.3096,  0.9646,  0.1079,  1.1262,  0.6131, 1.0951], ndmin=2).T\n",
        "\n",
        "plt.plot(x, y, 'o', label=\"Training Data\")\n",
        "plt.xlabel(\"x (attribute)\", fontsize=18)\n",
        "plt.ylabel(\"y (label)\", fontsize=18)\n",
        "plt.xlim(-1,1)\n",
        "plt.ylim(-0.5,2.5)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9jYiqJeavhD"
      },
      "source": [
        "Note that here we have obtained the transpose of two row vectors using `.T` instead of `np.transpose`. Both options are equivalent. The most important point is that `x` and `y` are two column vectors consisting of 10 rows and 1 column, i.e. 10x1 vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lmVZD3YbDcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e81c81d-69e5-4ee9-c2dc-e83268ce56dd"
      },
      "source": [
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 1)\n",
            "(10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgVr8pN-dAq0"
      },
      "source": [
        "Let's obtain the least squares solution for a simple linear model $f(x) = w_0 + w_1 x$. We need to calculate the design matrix first and then use the normal equation. We will show the calculations involved in the normal equations step by step:\n",
        "\n",
        "Step 1:  $X^TX$\n",
        "\n",
        "Step 2: $(X^TX)^{-1}$\n",
        "\n",
        "Step 3: $(X^TX)^{-1}X^T$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knhPStnWciuM"
      },
      "source": [
        "X = np.column_stack([np.ones(x.shape), x])\n",
        "print(\"The design matrix is:\\n\", X)\n",
        "\n",
        "XTX = np.dot(X.T, X) # Step 1\n",
        "XTX_inv = np.linalg.inv(XTX) # Step 2\n",
        "XTX_invXT = np.dot(XTX_inv, X.T) # Step 3\n",
        "\n",
        "w = np.dot(XTX_invXT, y)\n",
        "print(\"The 2 parameters of the least squares linear solution are\\n\", w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBXhwYgdDNfv"
      },
      "source": [
        "Now that we have the parameters $w_0$ and $w_1$ of the linear model, we can use them to predict the labels of 100 predictors uniformly spaced between -1 and 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHJlUMA0DXQC"
      },
      "source": [
        "x_LS = np.linspace(-1,1,100).T\n",
        "X_LS = np.column_stack([np.ones(x_LS.shape), x_LS])\n",
        "y_LS = np.dot(X_LS, w)\n",
        "\n",
        "\n",
        "plt.plot(x, y, 'o', label=\"Dataset\")\n",
        "plt.plot(x_LS, y_LS, label=\"Linear prediction\")\n",
        "plt.xlabel(\"x (attribute)\", fontsize=18)\n",
        "plt.ylabel(\"y (label)\", fontsize=18)\n",
        "plt.xlim(-1,1)\n",
        "plt.ylim(-0.5,2.5)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Uls_RA7F1iF"
      },
      "source": [
        "Let's do the same for a quadratic model. Note that our design matrix will include a new column with the squares of the predictors and instead of 2, we will have 3 parameters ($w_0$, $w_1$ and $w_2$):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dasliChiGNXS"
      },
      "source": [
        "X = np.column_stack([np.ones(x.shape), x, x**2]) # Note we are including powers up to 3, the square of the predictors\n",
        "print(\"The design matrix is:\\n\", X)\n",
        "\n",
        "XTX = np.dot(X.T, X)\n",
        "XTX_inv = np.linalg.inv(XTX)\n",
        "XTX_invXT = np.dot(XTX_inv, X.T)\n",
        "\n",
        "w = np.dot(XTX_invXT, y)\n",
        "print(\"The 3 parameters of the least squares quadratic solution are\\n\", w)\n",
        "\n",
        "X_LS = np.column_stack([np.ones(x_LS.shape), x_LS, x_LS**2])\n",
        "y_LS = np.dot(X_LS, w)\n",
        "\n",
        "\n",
        "plt.plot(x, y, 'o', label=\"Dataset\")\n",
        "plt.plot(x_LS, y_LS, label=\"Quadratic prediction\")\n",
        "plt.xlabel(\"x (attribute)\", fontsize=18)\n",
        "plt.ylabel(\"y (label)\", fontsize=18)\n",
        "plt.xlim(-1,1)\n",
        "plt.ylim(-0.5,2.5)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQQ3O7d7GiSM"
      },
      "source": [
        "The least squares solution for a cubic model consists of four parameters and will be obtained in an identical fashion:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp1ZAGXsGh7Q"
      },
      "source": [
        "X = np.column_stack([np.ones(x.shape), x, x**2, x**3]) # Note we are including powers up to 3, the cube of the predictors\n",
        "print(\"The design matrix is:\\n\", X)\n",
        "\n",
        "XTX = np.dot(X.T, X)\n",
        "XTX_inv = np.linalg.inv(XTX)\n",
        "XTX_invXT = np.dot(XTX_inv, X.T)\n",
        "\n",
        "w = np.dot(XTX_invXT, y)\n",
        "print(\"The 4 parameters of the least squares cubic solution are\\n\", w)\n",
        "\n",
        "X_LS = np.column_stack([np.ones(x_LS.shape), x_LS, x_LS**2, x_LS**3])\n",
        "y_LS = np.dot(X_LS, w)\n",
        "\n",
        "\n",
        "plt.plot(x, y, 'o', label=\"Dataset\")\n",
        "plt.plot(x_LS, y_LS, label=\"Cubic prediction\")\n",
        "plt.xlabel(\"x (attribute)\", fontsize=18)\n",
        "plt.ylabel(\"y (label)\", fontsize=18)\n",
        "plt.xlim(-1,1)\n",
        "plt.ylim(-0.5,2.5)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMuyfSvpHB2T"
      },
      "source": [
        "Finally, let's obtain the least squares solution of a polynomial of degree 4. However, lets put steps 1 to 3 into a single line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYwZumZTHFZc"
      },
      "source": [
        "\n",
        "\n",
        "X = np.column_stack([np.ones(x.shape), x, x**2, x**3, x**4]) # Note we are including powers up to 4!\n",
        "print(\"The design matrix is:\\n\", X)\n",
        "\n",
        "w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
        "print(\"The 5 parameters of the least squares 4-degree solution are\\n\", w)\n",
        "\n",
        "X_LS = np.column_stack([np.ones(x_LS.shape), x_LS, x_LS**2, x_LS**3, x_LS**4])\n",
        "y_LS = np.dot(X_LS, w)\n",
        "\n",
        "\n",
        "plt.plot(x, y, 'o', label=\"Dataset\")\n",
        "plt.plot(x_LS, y_LS, label=\"Degree 4 prediction\")\n",
        "plt.xlabel(\"x (attribute)\", fontsize=18)\n",
        "plt.ylabel(\"y (label)\", fontsize=18)\n",
        "plt.xlim(-1,1)\n",
        "plt.ylim(-0.5,2.5)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne5H1LL3jOcs"
      },
      "source": [
        "# Summary\n",
        "\n",
        "In this lab, we have covered several **operations** on NumPy arrays, specifically:\n",
        "\n",
        "- **Arithmetic operations**: `+`, `-`, `*`, `/`, `**`. \n",
        "- **Transposition**: Given a matrix `X`, its transpose is `np.transpose(X)` and also `X.T`.\n",
        "- **Column stacking**: Given several vectors with the same shape, for instance `x1`, `x2` and `x3`, they can be stacked together using `np.column_stack([x1, x2, x3])`.\n",
        "- **Matrix multiplication**: Given two matrices `X` and `Y` (with the right dimensions!), we can multiply them by typing `np.dot(X,Y)`.\n",
        "- **Matrix inversion**: The inverse of a matrix `X` can be obtained as `np.linalg.inv(X)`.\n",
        "\n",
        "With these basic operations, and after representing our data as NumPy arrays, we have writen code to:\n",
        "\n",
        "- Obtain the predicted label of a dataset consisting of any number of samples.\n",
        "- Obtain the least square solution for linear and polynomial models.\n",
        "\n",
        "When you are ready, have a go at the quiz. You will need to use one of the datasets from the regression exercises.\n"
      ]
    }
  ]
}